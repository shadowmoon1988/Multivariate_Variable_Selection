x/2
root = function(x,n){
old = x/2
tmp = 0
while ( abs(old-tmp)>0.000001 ) {
tmp = old
old = tmp*(n-1)/n +x/n*tmp^(1-n)
}
return(old)
}
root(2,2)
root(100,2)
?pois
?rpois
dpois(1,0.2)
dpois(0,0.2)
?rbinom
dbinom(1,10,0.02)
y=2003
m=13
d=10
W = (d+2*m+3*(m+1)/5+y+y/4-y/100+y/400)+1)
W = (d+2*m+3*(m+1)/5+y+y/4-y/100+y/400)+1
W
W % 7
W = (d+2*m+floor(3*(m+1)/5)+y+floor(y/4)-floor(y/100)+floor(y/400))+1
W
W % 7
W%7
W%%7
d = 11
W = (d+2*m+floor(3*(m+1)/5)+y+floor(y/4)-floor(y/100)+floor(y/400))+1
W%%7
?dnorm
score = function(ss){
ma = mean(ss)
sd = sd(ss)
w = dnorm(ss,ma,sd)
w = w/sum(w)
wma = sum(w*ss)
return(c(ma,wma))
}
26/7*60
26/9*60
ss1=c(rep(222,9),170)
score(ss1)
28*60/216.8
26*60/221.89
ss2 = c(rep(c(170,216,262),3).216)
ss2 = c(rep(c(170.8,216.8,262.8),3),216.8)
score(ss2)
dnorm(1)
dnorm(0)
require(huge)
require(extlasso)
require(optimx)
require(MRCE)
setwd("~/Brant Ying/Document of NUS/NUS/Research/Sparse")
source("Extended Lasso.R")
source("SLASSO.R")
source("Precision_estimation.R")
###require(NlcOptim)
#function part
##Via Gaussian transform to generate Nonparanormal
npn_gaussian.generator=function(data,mu=0.05,sigma=0.4,mu_v,sigma_v){
###inner use funciton for npn gaussian transform
f1 = function(t,m,s,m0,s0){pnorm((t-m0)/s0)*dnorm((t-m)/s)}
f2 = function(t,m,s,m0,s0){(pnorm((t-m0)/s0)-integrate(f1,-Inf,Inf,m=m,s=s,m0=m0,s0=s0)[[1]])^2*dnorm((t-m)/s)}
k=dim(data)[2]
output=matrix(0,dim(data)[1],k)
for (i in 1:k){
numerator= pnorm((data[,i]-mu)/sigma)-  integrate(f1,-Inf,Inf,m=mu_v[i],s=sigma_v[i],m0=mu,s0=sigma)[[1]]
dominator= sqrt(integrate(f2,-Inf,Inf,m=mu_v[i],s=sigma_v[i],m0=mu,s0=sigma)[[1]])
output[,i]=numerator/dominator
}
return(output)
}
##Via Power transform to generate Nonparanormal
npn_power.generator = function(data,alpha=3,mu_v,sigma_v){
###inner use funciton for npn power transform
f1 = function(t,m,s){abs(t-m)^(2*alpha)*dnorm((t-m)/s)}
k=dim(data)[2]
output=matrix(0,dim(data)[1],k)
for (i in 1:k){
numerator=sign(data[,i]-mu_v[i]) * abs(data[,i]-mu_v[i])^alpha
dominator=sqrt(integrate(f1,-Inf,Inf,m=mu_v[i],s=sigma_v[i])[[1]])
output[,i]=numerator/dominator
}
return(output)
}
## Marginal Estimation
# mest=function(input,data,thershold=1/(2*n),smooth=FALSE){
#     temp_fun = function(var){
#         temp=sum(ifelse(data<=var,1,0))/n
#         if (smooth & var>=min(data) & var<max(data) ){
#             lower=max(data[data<=var])
#             upper=min(data[data>var])
#             temp=temp+(var-lower)/(n*(upper-lower))
#         }
#         if (temp<thershold) {temp=thershold}
#         if (temp >1-thershold){temp=1-thershold}
#         return(qnorm(temp))
#     }
#
#     return(sapply(input,temp_fun))
# }
mest=function(input,data,bandwidth=n^(-0.2)){
temp_fun = function(var){
temp=sum(pnorm((var-data)/bandwidth))/n
thershold=1/2/n
if (temp<thershold) {temp=thershold}
if (temp >1-thershold){temp=1-thershold}
return(qnorm(temp))
}
return(sapply(input,temp_fun))
}
## 1st Derivative of Marginal Estimation
f_mest = function(var,data,bandwidth=n^(-0.2)){
output=sum(dnorm((var-data)/bandwidth))/(n*bandwidth) /dnorm(mest(var,data))
return(output)
}
## 2nd Derivative of Marginal Estimation
s_mest = function(var,data,bandwidth=n^(-0.2)){
temp=(var-data)/bandwidth
output = (sum(dnorm(temp))*mest(var,data)*f_mest(var,data) -sum(dnorm(temp)*temp/bandwidth))/
dnorm(mest(var,data))
return(output)
}
## Evaluation
evaluation = function(M1,M2){
nrow=dim(M1)[1]
ncol=dim(M1)[2]
FP=0
FN=0
r1=0
r2=0
for ( i in 1:nrow){
for ( j in 1:ncol){
if (M2[i,j] == 0){
if (M1[i,j]!=0) {
FP=FP+1
r1=r1+1
}
}
else {
r2=r2+1
if (M1[i,j]==0) {
FN=FN+1
} else {
r1=r1+1
}
}
}
}
output=c(FP/r1,1-FN/r2)
names(output)=c("FDR","PDR")
return(output)
}
n=100
q=10
p=100
set.seed(100)
E0=huge.generator(n,q,graph="random")
#E1=npn_gaussian.generator(E0$data,mu_v=rep(0,q),sigma_v=rep(1,q))
E1=npn_power.generator(E0$data,mu_v=rep(0,q),sigma_v=rep(1,q))
prob=floor(4*n^0.16)/p
B = matrix(abs(rnorm(p*q,0,0.1/qnorm(0.875)))+4*n^(-0.15),nrow=p,ncol=q)*
matrix(sample(c(-1,1),p*q,replace=TRUE,prob=c(0.4,0.6)),nrow=p,ncol=q)*
matrix(sample(0:1,p*q,replace=TRUE,prob=c(1-prob,prob)),nrow=p,ncol=q)
X=matrix(rnorm(n*p),nrow=n,ncol=p)
Y=X%*%B+E1
##Estimation
###seperate lasso
B_sepLasso=matrix(NA,nrow=p,ncol=q) #B_hat=solve(t(X)%*%X)%*%(t(X)%*%Y)
for (i in 1:q){
cv_lambda=cv.extlasso(X,Y[,i],family="normal",plot=FALSE)$lambda # CV get the best lambda
lasso_reg=extlasso(X,Y[,i],family="normal",intercept=FALSE)
B_sepLasso[,i]=predict(lasso_reg,mode="lambda",at=cv_lambda)[-1]
}
source("SLASSO.R")
B_hat = B_sepLasso# initial
SQ_B=matrix(NA,nrow=p,ncol=q)
e=Y-X%*%B_hat
###Get the nonparanormal correlation matrix
#r=huge.npn(e,npn.func="skeptic",verbose = FALSE)
#L=huge(r,method="glasso",cov.output=TRUE,verbose = FALSE)
#Omega=L$icov[[10]]#L$icov is the precision matrix. How to choose lambda?
trans_e=matrix(0,n,q)
for (i in 1:q){
trans_e[,i]=mest(e[,i],e[,i])
}
Omega=PME(trans_e)$Omega
i=1
Z=trans_e[,-i]
cond_mean=Z %*% (-Omega[-i,i]/Omega[i,i])
sys.time(SLasso2(Y[,i],e[,i],cond_mean))
Sys.time(SLasso2(Y[,i],e[,i],cond_mean))
SLasso2(Y[,i],e[,i],cond_mean)
system.time(SLasso2(Y[,i],e[,i],cond_mean))
?system.time
Seq_Lasso = function (X, Y, B_int,nonparanormal = TRUE){
B_old = B_int# initial
# SLasso with SKEPTIC
itr_count=0
B_new=matrix(NA,nrow=p,ncol=q)
trans_e=matrix(0,n,q)
repeat{
e=Y-X%*%B_old
if (nonparanormal){
for (i in 1:q){
trans_e[,i]=mest(e[,i],e[,i])
}
}
Omega=PME(trans_e)$Omega
### maximize the likelihood respect to b
### Sequential method
for (i in 1:q){
Z=trans_e[,-i]
cond_mean=Z %*% (-Omega[-i,i]/Omega[i,i])
B_new[,i]=SLasso2(Y[,i],e[,i],cond_mean)
}
itr_count=itr_count+1
print(itr_count)
if ( all(evaluation(B_new,B_old)==c(0,1)) | itr_count >= 100){ # need modify!!!!!!!!!!!!!!!!!!!!!!!!!
return(B_hat)
} else {
B_old=B_new
}
}
}
B_seq=Seq_Lasso(X,Y,B_sepLasso)
source("Coefficient_Estimation.R")
source("Precision_estimation.R")
Seq_Lasso = function (X, Y, B_int,nonparanormal = TRUE){
B_old = B_int# initial
# SLasso with SKEPTIC
itr_count=0
B_new=matrix(NA,nrow=p,ncol=q)
trans_e=matrix(0,n,q)
repeat{
e=Y-X%*%B_old
if (nonparanormal){
for (i in 1:q){
trans_e[,i]=mest(e[,i],e[,i])
}
}
Omega=PME(trans_e)$Omega
### maximize the likelihood respect to b
### Sequential method
for (i in 1:q){
Z=trans_e[,-i]
cond_mean=Z %*% (-Omega[-i,i]/Omega[i,i])
B_new[,i]=SLasso2(Y[,i],e[,i],cond_mean)
}
itr_count=itr_count+1
print(itr_count)
if ( all(evaluation(B_new,B_old)==c(0,1)) | itr_count >= 100){ # need modify!!!!!!!!!!!!!!!!!!!!!!!!!
return(B_hat)
} else {
B_old=B_new
}
}
}
B_seq=Seq_Lasso(X,Y,B_sepLasso)
evaluation(B_seq,B)
B_old=B_sepLasso
e=Y-X%*%B_old
if (nonparanormal){
for (i in 1:q){
trans_e[,i]=mest(e[,i],e[,i])
}
}
Omega=PME(trans_e)$Omega
for (i in 1:q){
trans_e[,i]=mest(e[,i],e[,i])
}
Omega=PME(trans_e)$Omega
Z=trans_e[,-i]
cond_mean=Z %*% (-Omega[-i,i]/Omega[i,i])
y=Y[,1]
e=e[,1]
cm=cond_mean
Fun = function(t,cond){ #1st negative deriavetive of obj function
cond_X=X[,cond]
dim(cond_X)=c(n,length(cond))
dim(t)=c(length(cond),1)
res=y-cond_X%*%t
sumvec=rep(0,length(cond))
for (i in 1:n){
sumvec = sumvec + ( mest(res[i],e) - cm[i] )*
f_mest(res[i],e) * cond_X[i,]
}
return(sumvec)
}
obj=function(t,cond=1:p){ # tell S
var=rep(0,p)
var[cond]=t
object=sum((mest(y-X%*%var,e)-cm)^2)
return(object)
}
OP = function(cond){
search = rep(0,length(cond))
search = search + Fun(search,cond)
return(search)
}
SA=1:p
S=NULL
beta=rep(0,p)
EBIC=Inf
Z=mest(y-X%*%beta,e)
R=-Inf
for (i in SA){
temp=t(X[,i])%*%(Z-cm)
if(temp > R){
R=temp
temp_s=i
}
}
S=c(S,temp_s)
SA=SA[SA!=temp_s]
##Estimate beta based on recent variable selection
beta[S]=OP(S)
beta[S]
optimize(f=obj,interval=c(-1000,1000),cond=S)$minimum
S
obj(6.61,S)
obj(732,S)
OP1 = function(cond){
search = rep(1,length(cond))
alpha = 0.1
itr=0
repeat{
new_search = search + alpha *Fun(search,cond)
itr =itr+1
if (  itr > 10  ) {
break
} else {
search=new_search
}
}
return(search)
}
OP1(S)
obj(1.04,S)
OP = function(cond){
search = rep(0,length(cond))
alpha = 0.2
itr=0
repeat{
new_search = search + alpha * Fun(search,cond)
itr =itr+1
if (  itr > 10  ) {
break
} else {
search=new_search
}
}
return(search)
}
beta[S]=OP(S)
beta[S]
obj(0.4,S)
n=100
q=10
p=100
set.seed(100)
E0=huge.generator(n,q,graph="random") # define own generator
#E1=npn_gaussian.generator(E0$data,mu_v=rep(0,q),sigma_v=rep(1,q))
E1=npn_power.generator(E0$data,mu_v=rep(0,q),sigma_v=rep(1,q))
prob=floor(4*n^0.16)/p
B = matrix(abs(rnorm(p*q,0,0.1/qnorm(0.875)))+4*n^(-0.15),nrow=p,ncol=q)*
matrix(sample(c(-1,1),p*q,replace=TRUE,prob=c(0.4,0.6)),nrow=p,ncol=q)*
matrix(sample(0:1,p*q,replace=TRUE,prob=c(1-prob,prob)),nrow=p,ncol=q)
X=matrix(rnorm(n*p),nrow=n,ncol=p) #design different X structure
Y=X%*%B+E1
##Estimation
###seperate lasso
B_sepLasso=matrix(NA,nrow=p,ncol=q) #B_hat=solve(t(X)%*%X)%*%(t(X)%*%Y)
for (i in 1:q){
cv_lambda=cv.extlasso(X,Y[,i],family="normal",plot=FALSE)$lambda # CV get the best lambda
lasso_reg=extlasso(X,Y[,i],family="normal",intercept=FALSE)
B_sepLasso[,i]=predict(lasso_reg,mode="lambda",at=cv_lambda)[-1]
}
B_seq=Seq_Lasso(X,Y,B_sepLasso)
evaluation(B_seq,B)
M1=B_seq
B1 = ifelse( M1 !=0 , 1, 0)
B1
M2 = B
B2 = ifelse( M2 !=0 , 1, 0)
numerator = sum (B1 & B2)
numerator
r = sum (B1 & B2) /c(sum (B1), sum (B2))
r
threshold=c(0.95,0.95)
result = all(r > threshold)
result
y
e
m
cm
obj=function(t,cond=1:p){ # tell S
var=rep(0,p)
var[cond]=t
object=sum((mest(y-X%*%var,e)-cm)^2)
return(object)
}
Fun = function(t,cond){ #1st negative deriavetive of obj function
cond_X=X[,cond]
dim(cond_X)=c(n,length(cond))
dim(t)=c(length(cond),1)
res=y-cond_X%*%t
sumvec=rep(0,length(cond))
for (i in 1:n){
sumvec = sumvec + ( mest(res[i],e) - cm[i] )*
f_mest(res[i],e) * cond_X[i,]
}
return(sumvec)
}
OP = function(cond){
search = rep(1,length(cond))
alpha = 0.1
itr=0
repeat{
new_search = search + alpha *Fun(search,cond)
itr =itr+1
if (  itr > 10  ) {
break
} else {
search=new_search
}
}
return(search)
}
SA=1:p
S=NULL
beta=rep(0,p)
EBIC=Inf
Z=mest(y-X%*%beta,e)
R=-Inf
for (i in SA){
temp=t(X[,i])%*%(Z-cm)
if(temp > R){
R=temp
temp_s=i
}
}
S=c(S,temp_s)
SA=SA[SA!=temp_s]
##Estimate beta based on recent variable selection
beta[S]=OP(S)
beta[S]
Z=mest(y-X%*%beta,e)
R=-Inf
for (i in SA){
temp=t(X[,i])%*%(Z-cm)
if(temp > R){
R=temp
temp_s=i
}
}
S=c(S,temp_s)
SA=SA[SA!=temp_s]
##Estimate beta based on recent variable selection
beta[S]=OP(S)
beta[S]
S
beta=rep(0,p)
SA=1:p
S=NULL
beta=rep(0,p)
Z=mest(y-X%*%beta,e,1/(2*n))
R=-Inf
for (i in SA){
temp=t(X[,i])%*%(Z-cm)
if(temp > R){
R=temp
temp_s=i
}
}
S=c(S,temp_s)
SA=SA[SA!=temp_s]
##Estimate beta based on recent variable selection
if (length(S) ==1){
beta[S]=optimize(f=obj,interval=c(-1000,1000),cond=S)$minimum
} else {
beta[S]=optim(rep(0,length(S)),fn=obj,cond=S)$par
#beta[S]=optimx(rep(0,length(S)),fn=obj2,cond=S)[1,1:length(S)]
}
Z=mest(y-X%*%beta,e,1/(2*n))
R=-Inf
for (i in SA){
temp=t(X[,i])%*%(Z-cm)
if(temp > R){
R=temp
temp_s=i
}
}
S=c(S,temp_s)
SA=SA[SA!=temp_s]
##Estimate beta based on recent variable selection
if (length(S) ==1){
beta[S]=optimize(f=obj,interval=c(-1000,1000),cond=S)$minimum
} else {
beta[S]=optim(rep(0,length(S)),fn=obj,cond=S)$par
#beta[S]=optimx(rep(0,length(S)),fn=obj2,cond=S)[1,1:length(S)]
}
beta[S]
S
?gc
rm(s_mest)
gc()
